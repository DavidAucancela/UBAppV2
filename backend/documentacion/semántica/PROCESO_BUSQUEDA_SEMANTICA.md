# üîç Proceso Detallado de B√∫squeda Sem√°ntica

## üìã Resumen Ejecutivo

El sistema de b√∫squeda sem√°ntica utiliza embeddings de OpenAI para convertir consultas en lenguaje natural en vectores de alta dimensionalidad, permitiendo encontrar env√≠os relevantes bas√°ndose en similitud sem√°ntica en lugar de coincidencias exactas de texto.

---

## üîÑ Flujo Completo del Proceso

### Fase 1: Recepci√≥n y Validaci√≥n de la Consulta

**Ubicaci√≥n**: `backend/apps/busqueda/views.py` - M√©todo `busqueda_semantica()` (L√≠nea 260-291)

1. **Endpoint**: `POST /api/busqueda/semantica/`
2. **Validaci√≥n**:
   - Verifica que el campo `texto` est√© presente
   - Valida par√°metros opcionales (`limite`, `modeloEmbedding`, `filtrosAdicionales`)
3. **Autenticaci√≥n**: Requiere token JWT v√°lido

**C√≥digo relevante**:
```python
consulta_texto = request.data.get('texto', '').strip()
if not consulta_texto:
    return Response({'error': 'El campo "texto" es requerido'}, 
                    status=status.HTTP_400_BAD_REQUEST)
```

---

### Fase 2: Procesamiento de la Consulta

**Ubicaci√≥n**: `backend/apps/busqueda/services.py` - M√©todo `buscar()` (L√≠nea 128-286)

#### 2.1 Validaci√≥n del Modelo de Embedding

```python
if modelo_embedding is None:
    modelo_embedding = EmbeddingService.get_modelo_default()  # text-embedding-3-small
else:
    modelo_embedding = EmbeddingService.validar_modelo(modelo_embedding)
```

**Modelos disponibles**:
- `text-embedding-3-small` (1536 dimensiones) - **Por defecto, m√°s econ√≥mico**
- `text-embedding-3-large` (3072 dimensiones) - Mayor precisi√≥n, m√°s costoso
- `text-embedding-ada-002` (1536 dimensiones) - Modelo legacy

#### 2.2 Procesamiento de Texto

**Ubicaci√≥n**: `backend/apps/busqueda/semantic/text_processor.py`

```python
consulta_procesada = TextProcessor.procesar_texto(consulta)
```

**Procesos aplicados**:
1. **Normalizaci√≥n**: Convertir a min√∫sculas
2. **Limpieza**: Eliminar caracteres especiales innecesarios
3. **Tokenizaci√≥n**: Dividir en palabras/tokens
4. **Lematizaci√≥n** (opcional): Reducir palabras a su ra√≠z

**Ejemplo**:
```
Input:  "Env√≠os entregados en Quito la semana pasada"
Output: "envios entregados en quito la semana pasada"
```

---

### Fase 3: Filtrado de Env√≠os por Permisos y Criterios

**Ubicaci√≥n**: `backend/apps/busqueda/services.py` - M√©todo `_obtener_envios_filtrados()` (L√≠nea 289-297)

```python
envios_queryset = BusquedaSemanticaService._obtener_envios_filtrados(
    usuario, filtros or {}
)
```

**Filtros aplicados**:
1. **Permisos del usuario**:
   - **Admin (rol=1)**: Ve todos los env√≠os
   - **Gerente (rol=2)**: Ve todos excepto admins
   - **Digitador (rol=3)**: Ve todos los env√≠os
   - **Comprador (rol=4)**: Solo sus propios env√≠os

2. **Filtros adicionales** (opcionales):
   - `fechaDesde`: Fecha de inicio
   - `fechaHasta`: Fecha de fin
   - `estado`: Estado del env√≠o (pendiente, entregado, etc.)
   - `ciudadDestino`: Ciudad de destino

**C√≥digo**:
```python
return envio_repository.filtrar_por_criterios_multiples(
    usuario=usuario,
    estado=filtros.get('estado'),
    fecha_desde=filtros.get('fechaDesde'),
    fecha_hasta=filtros.get('fechaHasta'),
    ciudad_destino=filtros.get('ciudadDestino')
)
```

---

### Fase 4: Generaci√≥n del Embedding de la Consulta

**Ubicaci√≥n**: `backend/apps/busqueda/semantic/embedding_service.py`

```python
embedding_resultado = EmbeddingService.generar_embedding(
    consulta_procesada, modelo_embedding
)
embedding_consulta = embedding_resultado['embedding']
tokens_consulta = embedding_resultado['tokens']
costo_consulta = embedding_resultado['costo']
```

#### 4.1 Proceso de Generaci√≥n

1. **Llamada a OpenAI API**:
   - Endpoint: `https://api.openai.com/v1/embeddings`
   - Modelo: `text-embedding-3-small` (por defecto)
   - Input: Texto procesado de la consulta

2. **Respuesta de OpenAI**:
   ```json
   {
     "data": [{
       "embedding": [0.123, -0.456, 0.789, ...],  // 1536 valores
       "index": 0
     }],
     "usage": {
       "prompt_tokens": 10,
       "total_tokens": 10
     }
   }
   ```

3. **Extracci√≥n de datos**:
   - **Embedding**: Vector de 1536 dimensiones (float32)
   - **Tokens**: N√∫mero de tokens utilizados
   - **Costo**: Calculado seg√∫n precio del modelo

#### 4.2 C√°lculo de Costo

**Precios por modelo** (por 1,000 tokens):
- `text-embedding-3-small`: $0.00002
- `text-embedding-3-large`: $0.00013
- `text-embedding-ada-002`: $0.0001

**F√≥rmula**:
```python
costo = (tokens / 1000) * precio_por_1k_tokens
```

---

### Fase 5: B√∫squeda de Env√≠os Similares

**Ubicaci√≥n**: `backend/apps/busqueda/services.py` - M√©todo `_buscar_envios_similares()` (L√≠nea 300-375)

#### 5.1 Obtenci√≥n de Embeddings de Env√≠os

```python
embeddings_envios = embedding_repository.obtener_embeddings_para_busqueda(
    envios_limitados,
    modelo=modelo_embedding,
    limite=MAX_ENVIOS_A_PROCESAR  # 300 env√≠os m√°ximo
)
```

**Optimizaci√≥n**:
- Solo se usan embeddings **ya generados** (no se generan en tiempo real)
- L√≠mite de 300 env√≠os para mantener rendimiento
- Los embeddings deben generarse previamente con: `python manage.py generar_embeddings`

**Estructura de datos**:
```python
embeddings_envios = [
    (envio_id, vector_embedding, objeto_envio),
    (envio_id, vector_embedding, objeto_envio),
    ...
]
```

#### 5.2 Obtenci√≥n de Textos Indexados

```python
envio_ids = [e[0] for e in embeddings_envios]
textos_indexados = embedding_repository.obtener_textos_indexados(envio_ids)
```

**Texto indexado**: Texto completo que se us√≥ para generar el embedding del env√≠o, incluyendo:
- HAWB (n√∫mero de env√≠o)
- Informaci√≥n del comprador
- Ciudad de destino
- Estado
- Descripci√≥n de productos
- Etc.

**Ejemplo**:
```
"HAWB: ABC123 | Comprador: Juan P√©rez | Ciudad: Quito | Estado: Entregado | 
Productos: Laptop, Mouse, Teclado | Peso: 5.5 kg"
```

---

### Fase 6: C√°lculo de Similitudes Vectoriales

**Ubicaci√≥n**: `backend/apps/busqueda/semantic/vector_search.py` - M√©todo `calcular_similitudes()` (L√≠nea 144-266)

#### 6.1 C√°lculo de M√∫ltiples M√©tricas

Para cada env√≠o, se calculan **4 m√©tricas de similitud**:

##### 6.1.1 Cosine Similarity (M√©trica Principal)

**F√≥rmula**: `cos(Œ∏) = (A ¬∑ B) / (||A|| √ó ||B||)`

**C√≥digo**:
```python
cosine_similarity = np.dot(consulta_vec, envio_vec) / (consulta_norm * envio_norm)
```

**Caracter√≠sticas**:
- Rango: `[-1, 1]`
- `1.0` = Vectores id√©nticos (m√°xima similitud)
- `0.0` = Vectores ortogonales (sin relaci√≥n)
- `-1.0` = Vectores opuestos

**Ventajas**:
- Normalizada (rango acotado)
- Invariante a escala (ignora magnitud)
- Est√°ndar en NLP

##### 6.1.2 Dot Product (Producto Punto)

**F√≥rmula**: `A ¬∑ B = Œ£(Ai √ó Bi)`

**C√≥digo**:
```python
dot_product = np.dot(consulta_vec, envio_vec)
```

**Caracter√≠sticas**:
- Rango: `[0, ‚àû]`
- Mayor valor = m√°s similar
- Depende de la magnitud de los vectores

**Limitaci√≥n**: Vectores m√°s largos tienen productos punto m√°s altos, incluso si no son m√°s similares sem√°nticamente.

##### 6.1.3 Euclidean Distance (Distancia Euclidiana)

**F√≥rmula**: `d = ‚àö(Œ£(Ai - Bi)¬≤)`

**C√≥digo**:
```python
euclidean_distance = np.linalg.norm(consulta_vec - envio_vec)
```

**Caracter√≠sticas**:
- Rango: `[0, ‚àû]`
- `0` = Vectores id√©nticos
- Mayor valor = m√°s diferente
- Mide distancia "en l√≠nea recta" en el espacio vectorial

**Limitaci√≥n**: Requiere normalizaci√≥n adicional para comparaci√≥n, sensible a la escala.

##### 6.1.4 Manhattan Distance (Distancia Manhattan)

**F√≥rmula**: `d = Œ£|Ai - Bi|`

**C√≥digo**:
```python
manhattan_distance = np.sum(np.abs(consulta_vec - envio_vec))
```

**Caracter√≠sticas**:
- Rango: `[0, ‚àû]`
- `0` = Vectores id√©nticos
- Mayor valor = m√°s diferente
- Suma de diferencias absolutas por componente

**Limitaci√≥n**: Similar a euclidean, menos sensible a outliers pero a√∫n requiere normalizaci√≥n.

#### 6.2 Boost por Coincidencias Exactas

**Ubicaci√≥n**: `backend/apps/busqueda/semantic/vector_search.py` (L√≠nea 200-247)

```python
coincidencias_score = TextProcessor.calcular_coincidencias_exactas(
    texto_consulta,
    textos_indexados[envio_id]
)

boost_exactas = coincidencias_score * boost_base
```

**Proceso**:
1. Busca coincidencias exactas de palabras entre la consulta y el texto indexado
2. Calcula un score de coincidencias (0.0 a 1.0)
3. Aplica un boost base (0.15 normal, 0.25 para productos)
4. Boost adicional para productos si hay coincidencias en descripciones

**Ejemplo**:
```
Consulta: "env√≠os a Quito"
Texto indexado: "HAWB: ABC123 | Ciudad: Quito | Estado: Entregado"
Coincidencias: "Quito" ‚Üí boost = 0.15
```

#### 6.3 Score Combinado

**F√≥rmula**: `score_combinado = (cosine + 1) / 2 + boost_exactas`

**C√≥digo**:
```python
cosine_normalizado = (cosine_similarity + 1) / 2  # Normalizar de [-1,1] a [0,1]
score_combinado = min(cosine_normalizado + boost_exactas, 1.0)
```

**Explicaci√≥n**:
1. Normaliza cosine similarity de `[-1, 1]` a `[0, 1]`
2. Suma el boost por coincidencias exactas
3. Limita el m√°ximo a 1.0

**Ventaja**: Combina similitud sem√°ntica (cosine) con coincidencias textuales (boost).

---

### Fase 7: Filtrado por Umbral

**Ubicaci√≥n**: `backend/apps/busqueda/semantic/vector_search.py` - M√©todo `aplicar_umbral()` (L√≠nea 311-358)

```python
umbral_base = 0.30 if es_consulta_productos else 0.35
resultados_filtrados = vector_search.aplicar_umbral(
    resultados_similitud,
    umbral_base=umbral_base,
    usar_adaptativo=True
)
```

#### 7.1 Umbral Adaptativo

Si hay m√°s de 3 resultados:
1. Ordena los scores de mayor a menor
2. Calcula el percentil 75 (25% m√°s bajos)
3. Usa el m√°ximo entre el percentil 75 y el umbral base

**Ejemplo**:
```
Scores: [0.92, 0.85, 0.78, 0.65, 0.45, 0.30]
Percentil 75 (√≠ndice 25%): 0.65
Umbral adaptativo: max(0.65, 0.35) = 0.65
Resultados filtrados: [0.92, 0.85, 0.78, 0.65]  # Solo >= 0.65
```

#### 7.2 Umbrales por Tipo de Consulta

- **Consultas normales**: Umbral base = 0.35
- **Consultas de productos**: Umbral base = 0.30 (m√°s permisivo)

---

### Fase 8: Ordenamiento de Resultados

**Ubicaci√≥n**: `backend/apps/busqueda/semantic/vector_search.py` - M√©todo `ordenar_por_metrica()` (L√≠nea 268-309)

```python
resultados_ordenados = vector_search.ordenar_por_metrica(
    resultados_filtrados,
    metrica='score_combinado',
    limite=limite
)
```

**Proceso**:
1. Ordena por `score_combinado` descendente (mayor a menor)
2. Limita a `limite` resultados (default: 20)

**Resultado**: Lista de env√≠os ordenados por relevancia sem√°ntica.

---

### Fase 9: Formateo de Resultados

**Ubicaci√≥n**: `backend/apps/busqueda/services.py` - M√©todo `_formatear_resultados()` (L√≠nea 377-435)

#### 9.1 Extracci√≥n de Fragmentos Relevantes

```python
fragmentos = TextProcessor.extraer_fragmentos(texto_consulta, texto_indexado)
```

Identifica las partes del texto indexado que son m√°s relevantes para la consulta.

**Ejemplo**:
```
Consulta: "env√≠os a Quito"
Fragmentos: ["Ciudad: Quito", "Comprador: Juan P√©rez (Quito)"]
```

#### 9.2 Generaci√≥n de Raz√≥n de Relevancia

```python
razon = TextProcessor.generar_razon_relevancia(
    texto_consulta, envio, resultado['score_combinado']
)
```

Genera una explicaci√≥n textual de por qu√© el env√≠o es relevante.

**Ejemplo**:
```
"Coincide con: ciudad Quito, estado entregado"
```

#### 9.3 An√°lisis Comparativo de M√©tricas

```python
analisis_metricas = BusquedaSemanticaService._generar_analisis_metricas(resultado)
```

Genera an√°lisis detallado de las 4 m√©tricas calculadas, justificando por qu√© cosine similarity es la mejor.

**Estructura del an√°lisis**:
```json
{
  "metricas": {
    "cosineSimilarity": {...},
    "dotProduct": {...},
    "euclideanDistance": {...},
    "manhattanDistance": {...},
    "scoreCombinado": {...}
  },
  "justificacion": {
    "metricaSeleccionada": "cosine_similarity",
    "razonBreve": "...",
    "comparacionRapida": {...},
    "conclusion": "..."
  }
}
```

#### 9.4 Estructura Final del Resultado

Cada resultado incluye:

```json
{
  "envio": {...},                    // Datos del env√≠o
  "puntuacionSimilitud": 0.9200,     // Score combinado
  "cosineSimilarity": 0.8500,        // Similitud coseno
  "dotProduct": 12.5000,             // Producto punto
  "euclideanDistance": 0.4500,        // Distancia euclidiana
  "manhattanDistance": 2.1000,      // Distancia Manhattan
  "scoreCombinado": 0.9200,         // Score final
  "boostExactas": 0.0700,           // Boost aplicado
  "analisisMetricas": {...},         // An√°lisis comparativo
  "fragmentosRelevantes": [...],     // Fragmentos destacados
  "razonRelevancia": "...",         // Explicaci√≥n textual
  "textoIndexado": "..."             // Texto usado para embedding
}
```

---

### Fase 10: Guardado en Historial

**Ubicaci√≥n**: `backend/apps/busqueda/services.py` - M√©todo `buscar()` (L√≠nea 226-243)

```python
busqueda = embedding_busqueda_repository.crear(
    usuario=usuario,
    consulta=consulta,  # Consulta original
    resultados_encontrados=len(resultados),
    tiempo_respuesta=tiempo_respuesta,
    filtros_aplicados=filtros,
    modelo_utilizado=modelo_embedding,
    costo_consulta=costo_consulta,
    tokens_utilizados=tokens_consulta,
    resultados_json=resultados
)

# Guardar el embedding de la consulta
busqueda.set_vector(embedding_consulta)
busqueda.save()
```

**Datos guardados**:
- Consulta original del usuario
- Embedding vectorial de la consulta
- N√∫mero de resultados encontrados
- Tiempo de respuesta (ms)
- Modelo utilizado
- Costo de la consulta (USD)
- Tokens utilizados
- Resultados completos (JSON)

---

### Fase 11: Logging y M√©tricas

**Ubicaci√≥n**: `backend/apps/busqueda/services.py` - M√©todo `buscar()` (L√≠nea 245-275)

```python
BaseService.log_operacion(
    operacion='buscar_semantica',
    entidad='BusquedaSemantica',
    usuario_id=usuario.id,
    detalles={...}
)

BaseService.log_metrica(
    metrica='busqueda_semantica_tiempo',
    valor=tiempo_respuesta,
    unidad='ms',
    ...
)
```

**M√©tricas registradas**:
- Tiempo de respuesta
- Costo de la consulta
- N√∫mero de resultados
- Modelo utilizado

---

### Fase 12: Respuesta Final

**Estructura de la respuesta**:

```json
{
  "consulta": "env√≠os entregados en Quito",
  "resultados": [
    {
      "envio": {...},
      "puntuacionSimilitud": 0.9200,
      "cosineSimilarity": 0.8500,
      "dotProduct": 12.5000,
      "euclideanDistance": 0.4500,
      "manhattanDistance": 2.1000,
      "scoreCombinado": 0.9200,
      "analisisMetricas": {
        "metricas": {...},
        "justificacion": {...}
      },
      "fragmentosRelevantes": [...],
      "razonRelevancia": "..."
    },
    ...
  ],
  "totalEncontrados": 5,
  "tiempoRespuesta": 156,
  "modeloUtilizado": "text-embedding-3-small",
  "costoConsulta": 0.0001,
  "tokensUtilizados": 10,
  "busquedaId": 123
}
```

---

## üìä Diagrama de Flujo

```
1. Usuario env√≠a consulta
   ‚Üì
2. Validaci√≥n y autenticaci√≥n
   ‚Üì
3. Procesamiento de texto (normalizaci√≥n, limpieza)
   ‚Üì
4. Filtrado de env√≠os (permisos + criterios)
   ‚Üì
5. Generaci√≥n de embedding de consulta (OpenAI API)
   ‚Üì
6. Obtenci√≥n de embeddings de env√≠os (base de datos)
   ‚Üì
7. C√°lculo de similitudes (4 m√©tricas)
   ‚Üì
8. Aplicaci√≥n de boost por coincidencias exactas
   ‚Üì
9. C√°lculo de score combinado
   ‚Üì
10. Filtrado por umbral adaptativo
   ‚Üì
11. Ordenamiento por score_combinado
   ‚Üì
12. Formateo de resultados (fragmentos, razones, an√°lisis)
   ‚Üì
13. Guardado en historial
   ‚Üì
14. Logging de m√©tricas
   ‚Üì
15. Respuesta al usuario
```

---

## üîë Puntos Clave del Proceso

### 1. **Optimizaci√≥n de Rendimiento**
- L√≠mite de 300 env√≠os a procesar
- Solo usa embeddings pre-generados (no genera en tiempo real)
- C√°lculo vectorial optimizado con NumPy

### 2. **M√©tricas M√∫ltiples**
- Se calculan 4 m√©tricas para cada resultado
- Cosine similarity es la m√©trica principal
- Score combinado mejora precisi√≥n con boost

### 3. **An√°lisis Autom√°tico**
- Cada resultado incluye an√°lisis comparativo de m√©tricas
- Justificaci√≥n autom√°tica de cosine similarity
- Informaci√≥n √∫til para documentaci√≥n acad√©mica

### 4. **Trazabilidad Completa**
- Cada b√∫squeda se guarda en historial
- Incluye embedding, m√©tricas, costo, tiempo
- Permite an√°lisis posterior y optimizaci√≥n

---

## üìù Referencias de C√≥digo

- **Endpoint principal**: `backend/apps/busqueda/views.py` - L√≠nea 260-291
- **L√≥gica de negocio**: `backend/apps/busqueda/services.py` - L√≠nea 128-286
- **C√°lculo de similitudes**: `backend/apps/busqueda/semantic/vector_search.py` - L√≠nea 144-266
- **Generaci√≥n de embeddings**: `backend/apps/busqueda/semantic/embedding_service.py`
- **Procesamiento de texto**: `backend/apps/busqueda/semantic/text_processor.py`
- **Formateo de resultados**: `backend/apps/busqueda/services.py` - L√≠nea 377-435
- **An√°lisis de m√©tricas**: `backend/apps/busqueda/services.py` - L√≠nea 440-690

---

**√öltima actualizaci√≥n**: Diciembre 2024

