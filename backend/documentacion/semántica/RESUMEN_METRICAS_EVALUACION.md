# Resumen: MÃ©tricas de EvaluaciÃ³n para BÃºsqueda SemÃ¡ntica

## âš ï¸ Respuesta Directa: Â¿CuÃ¡l NO es adecuada?

**âŒ RECALL NO ES ADECUADA** para evaluaciÃ³n en producciÃ³n.

**RazÃ³n:** Requiere conocer **TODOS** los resultados relevantes que existen en el sistema, lo cual es:
- Impracticable (tendrÃ­as que evaluar manualmente todos los envÃ­os para cada consulta)
- No escalable (con 215+ envÃ­os es inviable)
- Requiere ground truth completo (imposible en producciÃ³n)

---

## ðŸ“Š Resumen de MÃ©tricas

### 1. Precision (PrecisiÃ³n) âš ï¸ LIMITADA

**DefiniciÃ³n:** ProporciÃ³n de resultados recuperados que son relevantes.

**FÃ³rmula:**
```
Precision = Resultados Relevantes Recuperados / Total Resultados Recuperados
```

**Rango:** `[0, 1]`

**âœ… Ventajas:**
- FÃ¡cil de entender
- No requiere conocer todos los relevantes (solo los recuperados)

**âš ï¸ Limitaciones:**
- Requiere feedback para saber quÃ© es relevante
- No mide cobertura

**ðŸŽ¯ Variante Recomendada: Precision@K**
```
Precision@K = Relevantes en top K / K
```
EvalÃºa los primeros K resultados (mÃ¡s Ãºtil para bÃºsqueda).

---

### 2. Recall (Exhaustividad) âŒ NO ADECUADA

**DefiniciÃ³n:** ProporciÃ³n de resultados relevantes recuperados del total de relevantes existentes.

**FÃ³rmula:**
```
Recall = Resultados Relevantes Recuperados / Total Resultados Relevantes Existentes
```

**Rango:** `[0, 1]`

**âŒ PROBLEMA PRINCIPAL:**
- Requiere conocer **TODOS** los resultados relevantes que existen
- En bÃºsqueda semÃ¡ntica es **IMPOSIBLE** saber cuÃ¡ntos resultados relevantes existen
- RequerirÃ­a evaluar manualmente todos los envÃ­os del sistema para cada consulta

**CuÃ¡ndo SÃ se puede usar:**
- âœ… Solo en benchmarks acadÃ©micos con ground truth predefinido
- âœ… EvaluaciÃ³n de laboratorio con conjunto pequeÃ±o y controlado
- âŒ NO para producciÃ³n o evaluaciÃ³n continua

---

### 3. MRR (Mean Reciprocal Rank) âœ… ADECUADA - RECOMENDADA

**DefiniciÃ³n:** Inverso de la posiciÃ³n del primer resultado relevante, promediado sobre consultas.

**FÃ³rmula:**
```
MRR = (1/N) Ã— Î£(1 / posiciÃ³n_primer_relevante_i)
```

**Rango:** `[0, 1]`
- `1.0` = Primer resultado siempre es relevante (perfecto)
- `0.0` = Nunca hay resultados relevantes en los primeros lugares

**Ejemplo:**
```
Consulta 1: Primer relevante en posiciÃ³n 2 â†’ RR = 1/2 = 0.50
Consulta 2: Primer relevante en posiciÃ³n 1 â†’ RR = 1/1 = 1.00
Consulta 3: Primer relevante en posiciÃ³n 4 â†’ RR = 1/4 = 0.25

MRR = (0.50 + 1.00 + 0.25) / 3 = 0.58
```

**âœ… Ventajas (Ideal para BÃºsqueda):**
- Solo requiere el primer resultado relevante (no todos)
- FÃ¡cil de obtener con feedback (clicks, calificaciones)
- EvalÃºa calidad del ranking (experiencia del usuario)
- Escalable y prÃ¡ctico para producciÃ³n
- EstÃ¡ndar en sistemas de bÃºsqueda

**âš ï¸ LimitaciÃ³n:**
- Solo considera el primer resultado relevante (no importa si hay mÃ¡s despuÃ©s)

---

## ðŸŽ¯ MÃ©tricas Adicionales Recomendadas

### 4. NDCG@K (Normalized Discounted Cumulative Gain) â­â­ MUY RECOMENDADA

**DefiniciÃ³n:** EvalÃºa la calidad del ranking considerando posiciÃ³n y relevancia de mÃºltiples resultados.

**FÃ³rmula:**
```
DCG@K = Î£(i=1 to K) (relevancia_i / log2(i+1))
NDCG@K = DCG@K / IDCG@K
```

**âœ… Ventajas:**
- Considera mÃºltiples niveles de relevancia (0, 1, 2, 3...)
- Penaliza resultados relevantes en posiciones bajas
- Normalizado (comparables entre consultas)
- EstÃ¡ndar en informaciÃ³n retrieval

**Ejemplo:**
```
Resultados con relevancia:
1. relevancia 3 â†’ gain = 3/log2(2) = 3.00
2. relevancia 1 â†’ gain = 1/log2(3) = 0.63
3. relevancia 3 â†’ gain = 3/log2(4) = 1.50

DCG@3 = 3.00 + 0.63 + 1.50 = 5.13
IDCG@3 (ideal) = 5.13
NDCG@3 = 5.13 / 5.13 = 1.00 (perfecto)
```

---

## ðŸ“Š Comparativa RÃ¡pida

| MÃ©trica | Ground Truth Completo | ProducciÃ³n | Facilidad | Ranking |
|---------|----------------------|------------|-----------|---------|
| **MRR** | âŒ No | âœ…âœ…âœ… | âœ…âœ…âœ… | âœ… |
| **NDCG@K** | âš ï¸ Parcial | âœ…âœ…âœ… | âœ…âœ… | âœ…âœ…âœ… |
| **Precision@K** | âš ï¸ Parcial | âœ…âœ… | âœ…âœ…âœ… | âœ… |
| **Recall** | âŒâŒ SÃ­ | âŒ | âŒ | âŒ |

---

## âœ… RecomendaciÃ³n Final

### Para tu Sistema (Orden de Prioridad):

1. **MRR (Mean Reciprocal Rank)** â­â­â­
   - Implementar primero
   - Usar feedback implÃ­cito (clicks del usuario)
   - Ideal para bÃºsqueda semÃ¡ntica

2. **NDCG@10** â­â­â­
   - Implementar segundo
   - Usar feedback explÃ­cito (calificaciones) o implÃ­cito (tiempo de visualizaciÃ³n)
   - EvalÃºa ranking completo

3. **Precision@5 y Precision@10** â­â­
   - Implementar como complemento
   - EvalÃºa top K resultados

**NO usar:**
- âŒ **Recall:** Requiere ground truth completo (impracticable)
- âš ï¸ **MAP:** Similar a Recall, requiere ground truth completo

---

## ðŸ“ Cita para Tesis

> "Se seleccionaron MRR y NDCG@K como mÃ©tricas principales de evaluaciÃ³n. MRR mide la posiciÃ³n del primer resultado relevante, siendo ideal para bÃºsqueda donde el usuario busca encontrar informaciÃ³n rÃ¡pidamente. NDCG@K evalÃºa el ranking completo considerando mÃºltiples niveles de relevancia. Ambas mÃ©tricas pueden calcularse con feedback del usuario sin requerir ground truth completo, siendo prÃ¡cticas para producciÃ³n. Se descartÃ³ Recall porque requiere conocer todos los resultados relevantes existentes, lo cual es impracticable en producciÃ³n."

---

## ðŸ”§ ImplementaciÃ³n RÃ¡pida

### MRR con Clicks (Feedback ImplÃ­cito)

```python
def calcular_mrr(consultas_resultados_clicks):
    """
    consultas_resultados_clicks: [
        {
            'resultados': [HAW001, HAW002, HAW003, ...],
            'clicks': [HAW002]  # Usuario clickeÃ³ HAW002
        },
        ...
    ]
    """
    reciprocal_ranks = []
    for data in consultas_resultados_clicks:
        if data['clicks']:
            pos = data['resultados'].index(data['clicks'][0]) + 1
            reciprocal_ranks.append(1.0 / pos)
        else:
            reciprocal_ranks.append(0.0)
    return sum(reciprocal_ranks) / len(reciprocal_ranks)
```

### NDCG@K con Calificaciones

```python
import math

def calcular_ndcg(resultados, calificaciones, k=10):
    """
    resultados: Lista de IDs en orden de ranking
    calificaciones: Dict {resultado_id: relevancia (0-3)}
    """
    dcg = sum(calificaciones.get(r, 0) / math.log2(i+2) 
              for i, r in enumerate(resultados[:k]))
    relevancias_ideales = sorted(calificaciones.values(), reverse=True)[:k]
    idcg = sum(r / math.log2(i+2) for i, r in enumerate(relevancias_ideales))
    return dcg / idcg if idcg > 0 else 0.0
```

