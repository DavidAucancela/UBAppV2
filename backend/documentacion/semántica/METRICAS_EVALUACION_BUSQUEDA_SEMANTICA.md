# M√©tricas de Evaluaci√≥n para B√∫squeda Sem√°ntica

## üéØ Resumen Ejecutivo

Este documento explica las m√©tricas de evaluaci√≥n adecuadas para medir la calidad del sistema de b√∫squeda sem√°ntica, sus ventajas, limitaciones y c√≥mo implementarlas.

---

## ‚ö†Ô∏è An√°lisis de M√©tricas Propuestas

### M√©tricas Consideradas
1. **Precision (Precisi√≥n)** ‚ö†Ô∏è **LIMITADA** - Requiere ground truth
2. **Recall (Exhaustividad)** ‚ö†Ô∏è **LIMITADA** - Requiere ground truth completo
3. **MRR (Mean Reciprocal Rank)** ‚úÖ **ADECUADA** - Ideal para b√∫squeda

---

## üìä 1. Precision (Precisi√≥n)

### Definici√≥n
**Proporci√≥n de resultados recuperados que son relevantes.**

**F√≥rmula:**
```
Precision = (Resultados Relevantes Recuperados) / (Total Resultados Recuperados)
Precision = TP / (TP + FP)

Donde:
- TP (True Positives): Resultados recuperados que son relevantes
- FP (False Positives): Resultados recuperados que NO son relevantes
```

**Rango:** `[0, 1]`
- `1.0` = Todos los resultados son relevantes (perfecto)
- `0.0` = Ning√∫n resultado es relevante (muy malo)

### Ejemplo Pr√°ctico

```
Consulta: "env√≠os de celulares del mes anterior"
Resultados recuperados: 10 env√≠os

Evaluaci√≥n manual:
- 7 env√≠os son relevantes (tienen celulares del mes anterior)
- 3 env√≠os NO son relevantes (otros productos o fechas diferentes)

Precision = 7 / 10 = 0.70 (70%)
```

### ‚úÖ Ventajas

1. **F√°cil de entender:** "De lo que mostr√©, ¬øqu√© porcentaje es √∫til?"
2. **No requiere conocer todos los resultados relevantes:** Solo eval√∫a lo que se recuper√≥
3. **√ötil para evaluar calidad del ranking:** Especialmente Precision@K

### ‚ö†Ô∏è Limitaciones

1. **Requiere ground truth o feedback:** Necesitas saber qu√© resultados son relevantes
2. **No mide cobertura:** Puedes tener alta precision con pocos resultados
3. **Depende del tama√±o del conjunto recuperado:** M√°s resultados ‚Üí generalmente menor precision

### üéØ Precision@K (Variante Recomendada)

**Precisi√≥n considerando solo los primeros K resultados.**

**F√≥rmula:**
```
Precision@K = (Relevantes en top K) / K
```

**Ejemplo:**
```
Consulta: "env√≠os a Quito"
Resultados: [HAW001, HAW002, HAW003, HAW004, HAW005]
Relevantes: HAW001, HAW003, HAW005

Precision@1 = 1/1 = 1.00 (100%)
Precision@3 = 2/3 = 0.67 (67%)
Precision@5 = 3/5 = 0.60 (60%)
```

**‚úÖ Ventaja:** Eval√∫a qu√© tan bien est√°n ordenados los resultados m√°s importantes (top K).

---

## üìä 2. Recall (Exhaustividad)

### Definici√≥n
**Proporci√≥n de resultados relevantes que fueron recuperados del total de relevantes existentes.**

**F√≥rmula:**
```
Recall = (Resultados Relevantes Recuperados) / (Total Resultados Relevantes Existentes)
Recall = TP / (TP + FN)

Donde:
- TP (True Positives): Resultados recuperados que son relevantes
- FN (False Negatives): Resultados relevantes que NO fueron recuperados
```

**Rango:** `[0, 1]`
- `1.0` = Se recuperaron todos los resultados relevantes (perfecto)
- `0.0` = No se recuper√≥ ning√∫n resultado relevante (muy malo)

### Ejemplo Pr√°ctico

```
Consulta: "env√≠os de celulares del mes anterior"
Total de env√≠os relevantes en el sistema: 25
Resultados recuperados: 10
De los 10 recuperados, 7 son relevantes

Recall = 7 / 25 = 0.28 (28%)
```

### ‚úÖ Ventajas

1. **Mide cobertura completa:** Eval√∫a si el sistema encuentra todos los resultados relevantes
2. **√ötil para comparar algoritmos:** Permite ver qu√© sistema encuentra m√°s resultados

### ‚ö†Ô∏è Limitaciones Cr√≠ticas para B√∫squeda Sem√°ntica

1. **‚ùå PROBLEMA PRINCIPAL: Requiere conocer TODOS los resultados relevantes**
   - En b√∫squeda sem√°ntica, es imposible saber cu√°ntos resultados relevantes existen
   - No hay un "conjunto de verdad" (ground truth) predefinido
   - Requerir√≠a evaluar manualmente TODOS los env√≠os del sistema para cada consulta

2. **Impracticable a escala:**
   - Con 215 env√≠os en el sistema, tendr√≠as que evaluar todos para cada consulta
   - No escalable para producci√≥n

3. **Subjetivo:**
   - La "relevancia" es subjetiva y depende del usuario
   - Diferentes usuarios pueden tener diferentes criterios

### üéØ Cu√°ndo S√ç se puede usar Recall

**Solo en contextos controlados:**
- ‚úÖ Dataset de evaluaci√≥n con ground truth conocido (benchmarks acad√©micos)
- ‚úÖ Evaluaci√≥n de laboratorio con conjunto de pruebas peque√±o
- ‚úÖ Cuando tienes feedback completo del usuario para todas las consultas

**‚ùå NO adecuado para:**
- Evaluaci√≥n en producci√≥n
- Sistema en uso real
- Evaluaci√≥n continua del sistema

---

## üìä 3. MRR (Mean Reciprocal Rank) ‚≠ê RECOMENDADA

### Definici√≥n
**Inverso de la posici√≥n del primer resultado relevante, promediado sobre m√∫ltiples consultas.**

**F√≥rmula:**
```
MRR = (1/N) √ó Œ£(1 / posici√≥n_primer_relevante_i)

Donde:
- N = n√∫mero de consultas
- posici√≥n_primer_relevante_i = posici√≥n del primer resultado relevante en la consulta i
```

**Rango:** `[0, 1]`
- `1.0` = El primer resultado siempre es relevante (perfecto)
- `0.0` = Nunca hay resultados relevantes en los primeros lugares (muy malo)

### Ejemplo Pr√°ctico

```
Consulta 1: "env√≠os a Quito"
Resultados: [HAW001, HAW002, HAW003, HAW004, HAW005]
Relevantes: HAW002 (posici√≥n 2)
RR1 = 1/2 = 0.50

Consulta 2: "celulares del mes anterior"
Resultados: [HAW010, HAW011, HAW012]
Relevantes: HAW010 (posici√≥n 1)
RR2 = 1/1 = 1.00

Consulta 3: "productos electr√≥nicos"
Resultados: [HAW020, HAW021, HAW022, HAW023]
Relevantes: HAW023 (posici√≥n 4)
RR3 = 1/4 = 0.25

MRR = (0.50 + 1.00 + 0.25) / 3 = 0.58
```

### ‚úÖ Ventajas (Ideal para B√∫squeda Sem√°ntica)

1. **‚úÖ Solo requiere el primer resultado relevante:**
   - No necesitas evaluar todos los resultados
   - No necesitas conocer todos los relevantes existentes

2. **‚úÖ F√°cil de obtener con feedback del usuario:**
   - Puedes usar clicks (primer click = primer relevante)
   - Puedes usar calificaciones (primer resultado con rating alto)
   - Puedes usar interacciones (primer resultado que el usuario ve)

3. **‚úÖ Eval√∫a calidad del ranking:**
   - Mide qu√© tan bien est√° ordenado el primer resultado relevante
   - Refleja la experiencia del usuario (encuentra r√°pido lo que busca)

4. **‚úÖ Escalable:**
   - Funciona con cualquier cantidad de resultados
   - No requiere evaluaci√≥n completa del sistema

5. **‚úÖ Est√°ndar en b√∫squeda:**
   - Ampliamente usada en sistemas de recomendaci√≥n
   - M√©trica com√∫n en benchmarks de informaci√≥n retrieval

### ‚ö†Ô∏è Limitaciones

1. **Solo considera el primer resultado relevante:**
   - No importa si hay m√°s resultados relevantes despu√©s
   - No distingue entre tener 1 o 10 resultados relevantes

2. **Requiere al menos un resultado relevante:**
   - Si no hay resultados relevantes, MRR = 0 (no distingue entre consultas sin resultados vs resultados mal ordenados)

### üéØ Variantes de MRR

#### MRR@K
**Solo considera los primeros K resultados.**

```
MRR@K = (1/N) √ó Œ£(1 / min(posici√≥n_primer_relevante_i, K+1))

Si el primer relevante est√° en posici√≥n > K, se cuenta como si estuviera en K+1
```

**Ejemplo con K=5:**
```
Consulta 1: Primer relevante en posici√≥n 2 ‚Üí 1/2 = 0.50
Consulta 2: Primer relevante en posici√≥n 7 ‚Üí 1/6 = 0.17 (se cuenta como posici√≥n 6)
```

---

## üìä M√©tricas Adicionales Recomendadas

### 4. NDCG (Normalized Discounted Cumulative Gain) ‚≠ê MUY RECOMENDADA

### Definici√≥n
**Eval√∫a la calidad del ranking considerando la posici√≥n y relevancia de cada resultado.**

**F√≥rmula:**
```
DCG@K = Œ£(i=1 to K) (relevancia_i / log2(i+1))
NDCG@K = DCG@K / IDCG@K

Donde:
- relevancia_i = nivel de relevancia del resultado en posici√≥n i (0, 1, 2, 3, ...)
- IDCG = DCG ideal (mejor ranking posible)
```

**Rango:** `[0, 1]`
- `1.0` = Ranking perfecto
- `0.0` = Ranking muy malo

### Ventajas

1. **‚úÖ Considera m√∫ltiples niveles de relevancia:** Puedes usar 0, 1, 2, 3 (no relevante, poco, medio, muy relevante)
2. **‚úÖ Penaliza resultados relevantes en posiciones bajas:** M√°s realista
3. **‚úÖ Normalizado:** Comparables entre diferentes consultas
4. **‚úÖ Est√°ndar en informaci√≥n retrieval:** Usada en competencias acad√©micas

### Ejemplo

```
Consulta: "env√≠os a Quito"
Resultados con relevancia:
1. HAW001 - relevancia 3 (muy relevante) ‚Üí gain = 3/log2(2) = 3.00
2. HAW002 - relevancia 1 (poco relevante) ‚Üí gain = 1/log2(3) = 0.63
3. HAW003 - relevancia 3 (muy relevante) ‚Üí gain = 3/log2(4) = 1.50
4. HAW004 - relevancia 0 (no relevante) ‚Üí gain = 0/log2(5) = 0.00

DCG@4 = 3.00 + 0.63 + 1.50 + 0.00 = 5.13

IDCG@4 (ranking ideal): [3, 3, 1, 0] = 3.00 + 1.50 + 0.63 + 0.00 = 5.13

NDCG@4 = 5.13 / 5.13 = 1.00 (ranking perfecto)
```

---

### 5. MAP (Mean Average Precision)

### Definici√≥n
**Promedio de precision en cada posici√≥n donde hay un resultado relevante.**

**F√≥rmula:**
```
AP = (1/R) √ó Œ£(k=1 to n) Precision@k √ó relevante_k

Donde:
- R = n√∫mero total de resultados relevantes
- relevante_k = 1 si el resultado en posici√≥n k es relevante, 0 si no
- Precision@k = precision considerando los primeros k resultados

MAP = Promedio de AP sobre todas las consultas
```

**Ventajas:**
- ‚úÖ Considera m√∫ltiples resultados relevantes
- ‚úÖ Eval√∫a calidad del ranking completo
- ‚úÖ Est√°ndar en evaluaci√≥n de sistemas de b√∫squeda

**Limitaciones:**
- ‚ö†Ô∏è Requiere conocer todos los resultados relevantes (como Recall)

---

## üéØ Recomendaciones para tu Sistema

### ‚úÖ M√©tricas ADECUADAS (Orden de Prioridad)

1. **MRR (Mean Reciprocal Rank)** ‚≠ê‚≠ê‚≠ê
   - **Prioridad:** ALTA
   - **Raz√≥n:** Ideal para b√∫squeda, solo requiere primer resultado relevante
   - **Implementaci√≥n:** Usar feedback del usuario (clicks, calificaciones)

2. **NDCG@K** ‚≠ê‚≠ê‚≠ê
   - **Prioridad:** ALTA
   - **Raz√≥n:** Eval√∫a ranking completo con m√∫ltiples niveles de relevancia
   - **Implementaci√≥n:** Usar calificaciones de relevancia (0-3 o 0-5)

3. **Precision@K** ‚≠ê‚≠ê
   - **Prioridad:** MEDIA
   - **Raz√≥n:** √ötil para evaluar top K resultados
   - **Implementaci√≥n:** Feedback binario (relevante/no relevante)

### ‚ö†Ô∏è M√©tricas LIMITADAS

4. **Recall** ‚ùå
   - **Prioridad:** BAJA (solo para evaluaci√≥n controlada)
   - **Raz√≥n:** Requiere conocer TODOS los resultados relevantes (impracticable)
   - **Cu√°ndo usar:** Solo en benchmarks acad√©micos con ground truth

5. **MAP (Mean Average Precision)** ‚ö†Ô∏è
   - **Prioridad:** MEDIA-BAJA
   - **Raz√≥n:** Tambi√©n requiere ground truth completo
   - **Cu√°ndo usar:** Si tienes evaluaci√≥n exhaustiva con usuarios

---

## üìù Implementaci√≥n Pr√°ctica

### Opci√≥n 1: Feedback Impl√≠cito (Recomendado para Producci√≥n)

**Usar interacciones del usuario:**
- Clicks en resultados
- Tiempo de visualizaci√≥n
- Descargas/impresiones
- Navegaci√≥n (si clickea un resultado y luego busca otro, el primero no era relevante)

**Implementaci√≥n:**
```python
# Calcular MRR con clicks
def calcular_mrr_con_clicks(consultas_resultados_clicks):
    """
    consultas_resultados_clicks: [
        {
            'consulta': "env√≠os a Quito",
            'resultados': [HAW001, HAW002, HAW003, ...],
            'clicks': [HAW002]  # Usuario clicke√≥ HAW002
        },
        ...
    ]
    """
    reciprocal_ranks = []
    
    for consulta_data in consultas_resultados_clicks:
        resultados = consulta_data['resultados']
        clicks = consulta_data['clicks']
        
        if clicks:
            primer_click = clicks[0]
            posicion = resultados.index(primer_click) + 1  # +1 porque empieza en 1
            reciprocal_ranks.append(1.0 / posicion)
        else:
            reciprocal_ranks.append(0.0)  # No hubo clicks = no relevante
    
    return sum(reciprocal_ranks) / len(reciprocal_ranks)
```

### Opci√≥n 2: Feedback Expl√≠cito (Recomendado para Evaluaci√≥n)

**Usar calificaciones del usuario:**
- Bot√≥n "√ötil" / "No √∫til"
- Rating 1-5 estrellas
- Calificaci√≥n de relevancia (0-3)

**Implementaci√≥n:**
```python
# Calcular NDCG con calificaciones
def calcular_ndcg_con_calificaciones(resultados, calificaciones, k=10):
    """
    resultados: Lista de IDs de resultados en orden de ranking
    calificaciones: Dict {resultado_id: relevancia (0-3)}
    k: N√∫mero de resultados a considerar
    """
    dcg = 0.0
    for i, resultado_id in enumerate(resultados[:k], 1):
        relevancia = calificaciones.get(resultado_id, 0)
        dcg += relevancia / math.log2(i + 1)
    
    # IDCG: ordenar calificaciones de mayor a menor
    relevancias_ideales = sorted(calificaciones.values(), reverse=True)[:k]
    idcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(relevancias_ideales))
    
    return dcg / idcg if idcg > 0 else 0.0
```

### Opci√≥n 3: Evaluaci√≥n Manual (Solo para Pruebas)

**Evaluar manualmente un conjunto de consultas:**
- Seleccionar 20-50 consultas representativas
- Evaluar manualmente qu√© resultados son relevantes
- Calcular todas las m√©tricas

---

## üìä Resumen Comparativo

| M√©trica | Requiere Ground Truth | Adecuada para Producci√≥n | Facilidad Implementaci√≥n | Considera Ranking |
|---------|----------------------|-------------------------|-------------------------|-------------------|
| **MRR** | ‚ùå No (solo primer relevante) | ‚úÖ‚úÖ‚úÖ S√≠ | ‚úÖ‚úÖ‚úÖ F√°cil | ‚úÖ S√≠ (posici√≥n) |
| **NDCG** | ‚ö†Ô∏è Parcial (calificaciones) | ‚úÖ‚úÖ‚úÖ S√≠ | ‚úÖ‚úÖ Media | ‚úÖ‚úÖ‚úÖ S√≠ (m√∫ltiples posiciones) |
| **Precision@K** | ‚ö†Ô∏è Parcial (feedback top K) | ‚úÖ‚úÖ S√≠ | ‚úÖ‚úÖ‚úÖ F√°cil | ‚úÖ S√≠ (top K) |
| **Recall** | ‚ùå‚ùå S√≠ (todos los relevantes) | ‚ùå No | ‚ùå Dif√≠cil | ‚ùå No |
| **MAP** | ‚ùå‚ùå S√≠ (todos los relevantes) | ‚ö†Ô∏è Limitada | ‚ùå Dif√≠cil | ‚úÖ‚úÖ S√≠ |

---

## ‚úÖ Conclusi√≥n y Recomendaci√≥n Final

### Para tu Sistema de B√∫squeda Sem√°ntica:

**M√©tricas Recomendadas (en orden de prioridad):**

1. **MRR (Mean Reciprocal Rank)** ‚≠ê‚≠ê‚≠ê
   - Implementar primero
   - Usar feedback impl√≠cito (clicks)
   - F√°cil de implementar y muy adecuada para b√∫squeda

2. **NDCG@10** ‚≠ê‚≠ê‚≠ê
   - Implementar segundo
   - Usar feedback expl√≠cito (calificaciones) o impl√≠cito (tiempo de visualizaci√≥n)
   - Eval√∫a ranking completo

3. **Precision@5 y Precision@10** ‚≠ê‚≠ê
   - Implementar como complemento
   - √ötil para evaluar los resultados m√°s importantes

**NO Recomendar:**
- ‚ùå **Recall:** Requiere conocer TODOS los resultados relevantes (impracticable)
- ‚ö†Ô∏è **MAP:** Similar a Recall, requiere ground truth completo

### Justificaci√≥n para tu Tesis:

> "Se seleccionaron MRR y NDCG@K como m√©tricas principales de evaluaci√≥n porque: (1) MRR mide la calidad del ranking evaluando la posici√≥n del primer resultado relevante, siendo ideal para sistemas de b√∫squeda donde el usuario busca encontrar r√°pidamente informaci√≥n √∫til; (2) NDCG@K eval√∫a la calidad del ranking completo considerando m√∫ltiples niveles de relevancia y penalizando resultados relevantes en posiciones bajas; (3) Ambas m√©tricas pueden calcularse con feedback del usuario (clicks, calificaciones) sin requerir un ground truth completo, siendo pr√°cticas para evaluaci√≥n en producci√≥n. Se descart√≥ Recall debido a que requiere conocer todos los resultados relevantes existentes en el sistema, lo cual es impracticable en un entorno de producci√≥n con cientos de env√≠os."

---

## üîÑ Proceso de Evaluaci√≥n Implementado y Tabla Comparativa

### Flujo para identificar la eficiencia del panel sem√°ntico

1. **Definir pruebas controladas**  
   Crear consultas de prueba con su *ground truth* (lista de IDs de env√≠os relevantes) en el dashboard de m√©tricas (Pruebas Controladas Sem√°nticas).

2. **Ejecutar evaluaciones**  
   - Desde el **frontend**: pesta√±a "M√©tricas sem√°nticas del sistema" ‚Üí ejecutar cada prueba controlada.  
   - Desde **consola**:  
     `python manage.py evaluar_panel_semantico --ejecutar`  
     (ejecuta todas las pruebas activas y calcula MRR, nDCG@10, Precision@5).

3. **Ver resultados**  
   - **API**: `GET /api/metricas/metricas-semanticas/reporte-comparativo/?fecha_desde=&fecha_hasta=`  
     Devuelve `filas` (tabla por evaluaci√≥n) y `resumen` (promedios e interpretaci√≥n global).  
   - **Frontend**: pesta√±a "M√©tricas sem√°nticas del sistema" ‚Üí bloque **"Eficiencia del panel sem√°ntico"** con tabla comparativa y resumen.  
   - **Consola**: el comando `evaluar_panel_semantico` imprime la tabla en terminal; opci√≥n `--exportar reporte.csv` guarda CSV.

4. **Interpretaci√≥n**  
   - **MRR ‚â• 0.7**: Bueno (el primer resultado relevante suele estar arriba).  
   - **nDCG@10 ‚â• 0.6**: Bueno (ranking de calidad).  
   - **Precision@5 ‚â• 0.5**: Bueno (varios relevantes en el top 5).  
   El reporte asigna a cada fila y al resumen una etiqueta: *Bueno*, *Regular* o *Mejorable*.

### Ejemplo de tabla comparativa de resultados

| ID | Consulta                    | Fecha       | MRR   | nDCG@10 | Precision@5 | Interpretaci√≥n |
|----|-----------------------------|------------|-------|---------|-------------|----------------|
| 1  | env√≠os a Quito              | 2025-01-28 | 0.833 | 0.72    | 0.60        | Bueno          |
| 2  | celulares del mes anterior  | 2025-01-28 | 1.000 | 0.85    | 0.80        | Bueno          |
| 3  | productos electr√≥nicos      | 2025-01-28 | 0.250 | 0.41    | 0.20        | Mejorable      |
| **Resumen** | **3 evaluaciones**   |            | **0.69** | **0.66** | **0.53**   | **Aceptable**  |

- **Resumen**: total de evaluaciones, promedios de MRR / nDCG@10 / Precision@5 e interpretaci√≥n global (Eficiente / Aceptable / Mejorable).

### Ubicaci√≥n en el c√≥digo

- **C√°lculo de m√©tricas**: `backend/apps/metricas/utils.py` (`calcular_mrr`, `calcular_ndcg_k`, `calcular_precision_k`, `interpretar_metrica`).  
- **Reporte comparativo**: `backend/apps/metricas/repositories.py` ‚Üí `MetricaSemanticaRepository.obtener_reporte_comparativo`.  
- **API**: `GET .../metricas-semanticas/reporte-comparativo/`.  
- **Comando**: `python manage.py evaluar_panel_semantico [--ejecutar] [--exportar archivo.csv]`.

---

## üìö Referencias

1. **Manning et al. (2008)** - "Introduction to Information Retrieval"
   - Cap√≠tulo 8: Evaluation in information retrieval

2. **J√§rvelin & Kek√§l√§inen (2002)** - "Cumulated gain-based evaluation of IR techniques"
   - Introducci√≥n de NDCG

3. **Voorhees (1999)** - "The TREC-8 Question Answering Track Report"
   - Uso de MRR en evaluaci√≥n de sistemas de b√∫squeda

4. **Croft et al. (2010)** - "Search Engines: Information Retrieval in Practice"
   - Cap√≠tulo sobre evaluaci√≥n de sistemas de b√∫squeda

