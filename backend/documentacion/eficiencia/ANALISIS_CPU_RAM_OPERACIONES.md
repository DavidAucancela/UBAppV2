# üîç An√°lisis de Uso de CPU y RAM: Crear Env√≠o vs B√∫squeda Sem√°ntica

## üìä Resumen Ejecutivo

- **Crear Env√≠o**: Mayor uso de **CPU** debido a operaciones matem√°ticas intensivas con precisi√≥n decimal
- **B√∫squeda Sem√°ntica**: Mayor uso de **RAM** debido a carga masiva de vectores de embeddings en memoria

---

## üî¥ **Crear Env√≠o - Alto Uso de CPU**

### ¬øPor qu√© usa tanto CPU?

La operaci√≥n de crear un env√≠o requiere **c√°lculos matem√°ticos intensivos** con precisi√≥n decimal para garantizar exactitud financiera. Estos c√°lculos son **CPU-bound** (limitados por procesamiento) en lugar de I/O-bound (limitados por disco/red).

---

### üîç Desglose de Operaciones que Consumen CPU

#### 1. **C√°lculo de Costo del Servicio** ‚ö†Ô∏è **PRINCIPAL CONSUMIDOR**

```python
# backend/apps/archivos/models.py:176-226
def calcular_costo_servicio(self):
    # 1. Cargar todas las tarifas activas
    tarifas_activas = list(Tarifa.objects.filter(activa=True))
    
    # 2. Organizar por categor√≠a (iteraci√≥n sobre todas las tarifas)
    tarifas_por_categoria = {}
    for tarifa in tarifas_activas:
        if tarifa.categoria not in tarifas_por_categoria:
            tarifas_por_categoria[tarifa.categoria] = []
        tarifas_por_categoria[tarifa.categoria].append(tarifa)
    
    # 3. Para cada producto, buscar tarifa aplicable
    for producto in productos:
        for tarifa in tarifas_categoria:  # ‚ö†Ô∏è Bucle anidado
            if tarifa.peso_minimo <= producto.peso <= tarifa.peso_maximo:
                tarifa_aplicable = tarifa
                break
        
        # 4. C√°lculo con Decimal (MUY COSTOSO en CPU)
        costo_producto_decimal = Decimal(str(tarifa_aplicable.calcular_costo(producto.peso))) * Decimal(str(producto.cantidad))
        costo_total += costo_producto_decimal
```

**Operaciones que consumen CPU:**

| Operaci√≥n | Complejidad | Impacto CPU |
|-----------|-------------|-------------|
| Conversi√≥n `Decimal(str(...))` | O(1) por conversi√≥n | Alto (creaci√≥n de objetos Decimal) |
| Multiplicaci√≥n `Decimal * Decimal` | O(1) pero costoso | Alto (precisi√≥n arbitraria) |
| Comparaci√≥n `peso_minimo <= peso <= peso_maximo` | O(n) con n tarifas | Medio (bucle anidado) |
| Suma acumulativa `costo_total +=` | O(1) pero costoso | Alto (re-asignaci√≥n de Decimal) |

**Estimaci√≥n de operaciones por env√≠o:**
- **Tarifas a procesar**: 10-50 tarifas t√≠picamente
- **Productos por env√≠o**: 1-10 productos t√≠picamente
- **Operaciones Decimal**: ~100-500 operaciones
- **Tiempo CPU estimado**: 10-50 ms de CPU puro

#### 2. **Redondeo a 4 Decimales con `quantize()`**

```python
# backend/apps/archivos/models.py:223-226
if isinstance(costo_total, Decimal):
    return costo_total.quantize(Decimal('0.0001'))
```

**¬øPor qu√© es costoso?**
- `quantize()` requiere:
  1. Convertir el Decimal a string interno
  2. Aplicar redondeo bancario (round half to even)
  3. Validar precisi√≥n
  4. Crear nuevo objeto Decimal con precisi√≥n exacta
- **Tiempo CPU**: 1-5 ms por llamada

#### 3. **C√°lculo de Totales**

```python
# backend/apps/archivos/models.py:161-175
def calcular_totales(self):
    # Sumar pesos
    self.peso_total = sum(Decimal(str(p.peso)) * Decimal(str(p.cantidad)) for p in self.productos.all())
    
    # Sumar valores
    self.valor_total = sum(Decimal(str(p.valor)) * Decimal(str(p.cantidad)) for p in self.productos.all())
    
    # Recalcular costo (otra vez)
    self.costo_servicio = self.calcular_costo_servicio()
```

**Operaciones adicionales:**
- **Multiplicaciones Decimal**: N productos √ó 2 (peso y valor) = 2N operaciones
- **Sumas Decimal**: 2N sumas acumulativas
- **Tiempo CPU estimado**: 5-20 ms

#### 4. **Validaci√≥n de Cupo Anual**

```python
# backend/apps/archivos/services.py:82-85
if comprador.es_comprador:
    peso_total = float(data.get('peso_total', 0))
    UsuarioService.validar_cupo_disponible(comprador, peso_total)
```

**Operaciones:**
- Consulta agregada: `SUM(peso_total)` sobre todos los env√≠os del comprador en el a√±o
- Comparaci√≥n con l√≠mite anual
- **Tiempo CPU**: 5-15 ms (incluyendo I/O de BD)

---

### üìä Total de Uso de CPU Estimado

| Componente | Tiempo CPU Estimado | Porcentaje |
|------------|---------------------|------------|
| C√°lculo de costo del servicio | 10-50 ms | 50-70% |
| Redondeo con quantize() | 1-5 ms | 5-10% |
| C√°lculo de totales | 5-20 ms | 10-20% |
| Validaci√≥n de cupo | 5-15 ms | 5-10% |
| Operaciones de BD (I/O) | 10-30 ms | 10-20% |
| **TOTAL CPU** | **31-120 ms** | **100%** |

**Nota**: El tiempo total de respuesta es mayor (~200-500 ms) porque incluye I/O de red y BD, pero el **uso de CPU** se concentra en estas operaciones matem√°ticas.

---

### ‚öôÔ∏è ¬øPor qu√© Decimal es tan costoso?

**Decimal vs Float:**

| Caracter√≠stica | Float | Decimal |
|----------------|-------|---------|
| Precisi√≥n | ~15 d√≠gitos (binario) | Precisi√≥n arbitraria |
| Velocidad | R√°pido (hardware) | Lento (software) |
| Uso de memoria | 8 bytes | 28-80 bytes |
| Operaciones CPU | 1-10 ciclos | 100-1000 ciclos |

**Decimal en Python:**
- Implementado en **software puro** (no hardware)
- Usa aritm√©tica de **precisi√≥n arbitraria**
- Cada operaci√≥n crea **nuevos objetos**
- Requiere **gesti√≥n de memoria** intensiva

**Ejemplo de costo:**
```python
# Float: ~1 ciclo de CPU
resultado = 5.5 * 2.3  # Operaci√≥n en hardware

# Decimal: ~500 ciclos de CPU
resultado = Decimal('5.5') * Decimal('2.3')  # Operaci√≥n en software
```

---

### ‚úÖ Por qu√© se usa Decimal (justificaci√≥n)

**Raz√≥n principal**: **Precisi√≥n financiera**
- Los errores de redondeo en operaciones financieras son **inaceptables**
- Un error de $0.01 en millones de transacciones = miles de d√≥lares
- `Decimal` garantiza **precisi√≥n exacta** en c√°lculos monetarios

**Alternativas consideradas:**
- ‚ùå `float`: P√©rdida de precisi√≥n en c√°lculos repetidos
- ‚úÖ `Decimal`: Precisi√≥n exacta, costo aceptable

---

## üíæ **B√∫squeda Sem√°ntica - Alto Uso de RAM**

### ¬øPor qu√© usa tanta RAM?

La b√∫squeda sem√°ntica carga **miles de vectores de embeddings** en memoria simult√°neamente para realizar c√°lculos de similitud. Cada embedding es un vector grande de n√∫meros flotantes que consume memoria significativa.

---

### üîç Desglose de Uso de Memoria

#### 1. **Embeddings de Env√≠os en Memoria** ‚ö†Ô∏è **PRINCIPAL CONSUMIDOR**

```python
# backend/apps/busqueda/services.py:480-484
embeddings_envios = embedding_repository.obtener_embeddings_para_busqueda(
    envios_limitados,
    modelo=modelo_embedding,
    limite=MAX_ENVIOS_A_PROCESAR  # 1000 env√≠os
)
```

**Tama√±o de cada embedding:**
- **Dimensiones**: 1536 (modelo `text-embedding-3-small`) o 3072 (`text-embedding-3-large`)
- **Tipo de dato**: `float32` (4 bytes por n√∫mero)
- **Tama√±o por embedding**: 1536 √ó 4 bytes = **6,144 bytes = ~6 KB**

**Con 1000 env√≠os (l√≠mite actual):**
- **Memoria de vectores**: 1000 √ó 6 KB = **6 MB** (solo vectores)

#### 2. **Conversi√≥n a Arrays NumPy**

```python
# backend/apps/busqueda/semantic/vector_search.py:179-180
consulta_vec = np.array(embedding_consulta, dtype=np.float32)
matriz_envios = np.array(vectores_envios, dtype=np.float32)  # ‚ö†Ô∏è MATRIZ GRANDE
```

**Memoria adicional:**
- **Array de consulta**: 1536 √ó 4 bytes = **6 KB**
- **Matriz de env√≠os**: 1000 √ó 1536 √ó 4 bytes = **6 MB**
- **TOTAL arrays NumPy**: ~6 MB

**Nota**: NumPy puede duplicar memoria temporalmente durante conversiones.

#### 3. **Arrays Intermedios para C√°lculos**

```python
# backend/apps/busqueda/semantic/vector_search.py:189-205
normas_envios = np.linalg.norm(matriz_envios, axis=1)  # Array 1000 √ó 4 bytes = 4 KB
dot_products = np.dot(matriz_envios, consulta_vec)  # Array 1000 √ó 4 bytes = 4 KB
diferencias = matriz_envios - consulta_vec  # Matriz 1000 √ó 1536 √ó 4 bytes = 6 MB
euclidean_distances = np.linalg.norm(diferencias, axis=1)  # Array 1000 √ó 4 bytes = 4 KB
manhattan_distances = np.sum(np.abs(diferencias), axis=1)  # Array 1000 √ó 4 bytes = 4 KB
```

**Memoria temporal durante c√°lculos:**
- **Matriz de diferencias**: 6 MB
- **Arrays de resultados**: ~20 KB
- **TOTAL temporal**: ~6 MB (se libera despu√©s)

#### 4. **Objetos de Env√≠o y Textos Indexados**

```python
# backend/apps/busqueda/services.py:516-517
envio_ids = [e[0] for e in embeddings_envios]
textos_indexados = embedding_repository.obtener_textos_indexados(envio_ids)
```

**Memoria adicional:**
- **Lista de IDs**: 1000 √ó 8 bytes = **8 KB**
- **Diccionario de textos**: ~1-5 KB por texto √ó 1000 = **1-5 MB**
- **Objetos Envio en memoria**: ~500 bytes √ó 1000 = **500 KB**

---

### üìä Total de Uso de RAM Estimado

| Componente | Memoria Estimada | Porcentaje |
|------------|------------------|------------|
| **Embeddings (vectores originales)** | **6 MB** | **45-50%** |
| Arrays NumPy (matriz_envios) | 6 MB | 45-50% |
| Arrays intermedios (c√°lculos) | 6 MB (temporal) | N/A |
| Textos indexados | 1-5 MB | 10-30% |
| Objetos Envio | 500 KB | 3-5% |
| IDs y metadata | ~20 KB | <1% |
| **TOTAL PICO** | **~13-19 MB** | **100%** |
| **TOTAL ESTABLE** | **~7-11 MB** | **100%** |

**Nota**: Durante los c√°lculos, la memoria puede alcanzar un **pico de 19 MB** debido a arrays temporales. Despu√©s, se libera ~6 MB, quedando **13 MB estables**.

---

### üìà Relaci√≥n entre Cantidad de Env√≠os y RAM

| Env√≠os Procesados | RAM Estimada (pico) | RAM Estimada (estable) |
|-------------------|---------------------|------------------------|
| 100 env√≠os | ~2 MB | ~1.5 MB |
| 500 env√≠os | ~10 MB | ~7 MB |
| **1000 env√≠os (l√≠mite actual)** | **~19 MB** | **~13 MB** |
| 2000 env√≠os | ~38 MB | ~26 MB |
| 5000 env√≠os | ~95 MB | ~65 MB |

**L√≠mite actual**: `MAX_ENVIOS_A_PROCESAR = 1000` (l√≠nea 466 de `services.py`)

---

### ‚öôÔ∏è ¬øPor qu√© se cargan tantos embeddings?

**Raz√≥n principal**: **C√°lculos vectoriales eficientes**

Para calcular similitudes de manera eficiente, NumPy procesa **todos los vectores simult√°neamente** usando operaciones vectorizadas:

```python
# ‚ùå INEFICIENTE: Procesar uno por uno
for envio in embeddings:
    similitud = calcular_similitud(consulta, envio)  # Lento: 1000 iteraciones

# ‚úÖ EFICIENTE: Procesar todos a la vez
matriz = np.array([envio for envio in embeddings])
similitudes = np.dot(matriz, consulta)  # R√°pido: 1 operaci√≥n vectorizada
```

**Ventaja**: Las operaciones vectorizadas son **100-1000x m√°s r√°pidas** que bucles, pero requieren **toda la memoria a la vez**.

---

### üîÑ Optimizaciones Implementadas

#### 1. **L√≠mite de Env√≠os a Procesar**
```python
# backend/apps/busqueda/services.py:466
MAX_ENVIOS_A_PROCESAR = 1000
```
- Evita cargar m√°s de 1000 embeddings
- Limita RAM a ~13-19 MB m√°ximo

#### 2. **Uso de float32 en lugar de float64**
```python
# backend/apps/busqueda/semantic/vector_search.py:179-180
consulta_vec = np.array(embedding_consulta, dtype=np.float32)
matriz_envios = np.array(vectores_envios, dtype=np.float32)
```
- **Reducci√≥n de memoria**: 50% (4 bytes vs 8 bytes por float)
- **Impacto en precisi√≥n**: M√≠nimo (suficiente para similitudes)

#### 3. **Operaciones Vectorizadas**
```python
# backend/apps/busqueda/semantic/vector_search.py:189-205
normas_envios = np.linalg.norm(matriz_envios, axis=1)  # Vectorizado
dot_products = np.dot(matriz_envios, consulta_vec)  # Vectorizado
```
- **Velocidad**: 100-1000x m√°s r√°pido que bucles
- **Memoria**: Se reutilizan arrays eficientemente

---

### üí° Optimizaciones Futuras Sugeridas

#### 1. **√çndices Vectoriales Especializados** (Recomendado)
- **Pinecone**, **Weaviate**, o **Qdrant**
- **Ventaja**: No carga todos los embeddings en memoria
- **Reducci√≥n estimada**: 90-95% de RAM (solo carga resultados finales)
- **Mejora de velocidad**: 10-100x m√°s r√°pido en b√∫squedas

#### 2. **Procesamiento por Lotes (Chunking)**
```python
# Procesar en lotes de 500 en lugar de 1000
CHUNK_SIZE = 500
for i in range(0, len(embeddings), CHUNK_SIZE):
    chunk = embeddings[i:i+CHUNK_SIZE]
    calcular_similitudes(consulta, chunk)
```
- **Reducci√≥n de RAM**: 50%
- **Impacto en velocidad**: 10-20% m√°s lento (m√∫ltiples pasadas)

#### 3. **Cach√© de Resultados Frecuentes**
- Almacenar resultados de b√∫squedas comunes en cache
- **Reducci√≥n de RAM**: 100% para b√∫squedas cacheadas
- **Mejora de velocidad**: 1000x m√°s r√°pido (cache hit)

---

## üìä Comparativa General

| Operaci√≥n | CPU (ms) | RAM (MB) | Tipo de Carga |
|-----------|----------|----------|---------------|
| **Crear Env√≠o** | **31-120 ms** | **1-5 MB** | **CPU-bound** |
| **B√∫squeda Sem√°ntica** | **10-50 ms** | **13-19 MB** | **Memory-bound** |
| Login | 5-20 ms | 0.5-1 MB | Mixed |
| Otras operaciones (GET) | 1-10 ms | 0.5-2 MB | I/O-bound |

---

## üéØ Conclusi√≥n

### **Crear Env√≠o - Alto CPU:**
‚úÖ **Justificado**: Precisi√≥n financiera requiere operaciones Decimal costosas
- **Impacto**: Aceptable (31-120 ms de CPU)
- **Optimizaci√≥n futura**: Considerar cache de tarifas por categor√≠a

### **B√∫squeda Sem√°ntica - Alto RAM:**
‚úÖ **Justificado**: C√°lculos vectoriales eficientes requieren carga masiva
- **Impacto**: Aceptable (13-19 MB para 1000 env√≠os)
- **Optimizaci√≥n futura**: Implementar √≠ndices vectoriales especializados (Pinecone/Weaviate)

Ambos casos representan **trade-offs necesarios** entre rendimiento y funcionalidad. Las optimizaciones sugeridas pueden reducir estos consumos si se convierten en un problema en producci√≥n.