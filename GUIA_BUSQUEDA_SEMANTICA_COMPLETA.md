# üß† Gu√≠a Completa de B√∫squeda Sem√°ntica - Universal Box

## üìã Tabla de Contenidos
1. [Descripci√≥n General](#descripci√≥n-general)
2. [Arquitectura del Sistema](#arquitectura-del-sistema)
3. [Instalaci√≥n y Configuraci√≥n](#instalaci√≥n-y-configuraci√≥n)
4. [Uso del Sistema](#uso-del-sistema)
5. [M√©tricas de Similitud](#m√©tricas-de-similitud)
6. [Generaci√≥n de Embeddings](#generaci√≥n-de-embeddings)
7. [Optimizaci√≥n y Mejores Pr√°cticas](#optimizaci√≥n-y-mejores-pr√°cticas)

---

## üéØ Descripci√≥n General

Sistema de b√∫squeda sem√°ntica completo que permite encontrar env√≠os usando lenguaje natural, utilizando **OpenAI Embeddings** y **PostgreSQL con pgvector** para almacenamiento y c√°lculo de similitudes vectoriales.

### Caracter√≠sticas Principales

‚úÖ **Generaci√≥n autom√°tica de embeddings** al crear env√≠os  
‚úÖ **M√∫ltiples m√©tricas de similitud** (Cosine, Dot Product, Euclidean, Manhattan)  
‚úÖ **B√∫squeda en lenguaje natural** con IA  
‚úÖ **Almacenamiento vectorial nativo** con pgvector  
‚úÖ **M√©tricas de rendimiento** (precisi√≥n, costo, velocidad)  
‚úÖ **Interfaz moderna** en Angular con visualizaci√≥n de resultados  

---

## üèóÔ∏è Arquitectura del Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         FRONTEND (Angular)                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ  Busqueda Semantica Component                          ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - Input de consulta en lenguaje natural              ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - Visualizaci√≥n de resultados con m√©tricas           ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - Configuraci√≥n de modelos y umbrales                ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ HTTP/REST API
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      BACKEND (Django REST)                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ  BusquedaViewSet                                       ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - /api/busqueda/semantica/ (POST)                    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - Generaci√≥n de embedding de consulta                ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - C√°lculo de similitudes m√∫ltiples                   ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ  utils_embeddings.py                                   ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - generar_embedding_envio()                          ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - calcular_similitudes()                             ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - ordenar_por_metrica()                              ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ               ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   OpenAI API ‚îÇ  ‚îÇ  PostgreSQL +  ‚îÇ
        ‚îÇ   Embeddings ‚îÇ  ‚îÇ    pgvector    ‚îÇ
        ‚îÇ              ‚îÇ  ‚îÇ                ‚îÇ
        ‚îÇ text-emb-3-  ‚îÇ  ‚îÇ  EnvioEmbedding‚îÇ
        ‚îÇ    small     ‚îÇ  ‚îÇ  VectorField   ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Flujo de Datos

```
1. CARGA DE ENV√çO (Manual/Excel)
   ‚Üì
2. Crear Envio ‚Üí generar_embedding_envio()
   ‚Üì
3. Generar texto descriptivo del env√≠o
   ‚Üì
4. Llamada a OpenAI API ‚Üí Embedding (1536 dims)
   ‚Üì
5. Guardar en EnvioEmbedding (VectorField pgvector)

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

1. B√öSQUEDA SEM√ÅNTICA
   ‚Üì
2. Usuario ingresa consulta: "env√≠os pesados a Quito"
   ‚Üì
3. Generar embedding de consulta (OpenAI)
   ‚Üì
4. Buscar env√≠os con embeddings en BD
   ‚Üì
5. Calcular similitudes:
      - Cosine Similarity
      - Dot Product
      - Euclidean Distance
      - Manhattan Distance
   ‚Üì
6. Filtrar por umbral (ej: cosine >= 0.3)
   ‚Üì
7. Ordenar por m√©trica seleccionada
   ‚Üì
8. Retornar top N resultados con m√©tricas
```

---

## üöÄ Instalaci√≥n y Configuraci√≥n

### 1. Requisitos Previos

```bash
- Python 3.11+
- PostgreSQL 14+
- Node.js 18+
- API Key de OpenAI
```

### 2. Configurar Backend

#### 2.1 Instalar dependencias

```bash
cd backend
pip install -r requirements.txt
```

**Nuevas dependencias incluidas:**
- `psycopg2-binary==2.9.9` - Driver PostgreSQL
- `pgvector==0.2.5` - Soporte de vectores en Postgres
- `openai==1.12.0` - Cliente OpenAI
- `numpy==1.26.4` - C√°lculos vectoriales

#### 2.2 Configurar PostgreSQL con pgvector

**Opci√≥n A: Instalar extensi√≥n manualmente**

```bash
# Ubuntu/Debian
sudo apt-get install postgresql-14-pgvector

# macOS
brew install pgvector

# Windows: Descargar desde https://github.com/pgvector/pgvector/releases
```

**Conectar a PostgreSQL y habilitar:**

```sql
-- Conectar a tu base de datos
psql -U postgres -d equityDB

-- Habilitar extensi√≥n
CREATE EXTENSION IF NOT EXISTS vector;

-- Verificar
SELECT * FROM pg_extension WHERE extname = 'vector';
```

**Opci√≥n B: Usar Supabase** (ya tiene pgvector incluido)

```bash
# Solo necesitas la URL de conexi√≥n
```

#### 2.3 Configurar variables de entorno

Crear/editar `backend/.env`:

```env
# Base de datos
DB_NAME=equityDB
DB_USER=postgres
DB_PASSWORD=tu_password
DB_HOST=localhost
DB_PORT=5432

# OpenAI API
OPENAI_API_KEY=sk-proj-tu-key-de-openai-aqui
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_EMBEDDING_DIMENSIONS=1536

# Django
SECRET_KEY=tu-secret-key
DEBUG=True
```

#### 2.4 Ejecutar migraciones

```bash
cd backend

# Migraci√≥n para habilitar pgvector
python manage.py migrate busqueda 0006_habilitar_pgvector

# Migraci√≥n para actualizar modelos
python manage.py migrate busqueda 0007_actualizar_embedding_pgvector

# Todas las migraciones
python manage.py migrate
```

#### 2.5 Generar embeddings para env√≠os existentes

```bash
# Generar embeddings para todos los env√≠os sin embedding
python manage.py generar_embeddings_masivo

# Con opciones avanzadas
python manage.py generar_embeddings_masivo --modelo text-embedding-3-small --batch-size 50 --delay 0.1

# Forzar regeneraci√≥n de todos
python manage.py generar_embeddings_masivo --forzar

# Procesar solo un env√≠o espec√≠fico
python manage.py generar_embeddings_masivo --hawb ABC123456

# Limitar cantidad (para pruebas)
python manage.py generar_embeddings_masivo --limite 10
```

**Par√°metros del comando:**
- `--forzar`: Regenera embeddings existentes
- `--modelo`: Modelo OpenAI (text-embedding-3-small, text-embedding-3-large)
- `--limite`: N√∫mero m√°ximo de env√≠os a procesar
- `--hawb`: HAWB espec√≠fico a procesar
- `--batch-size`: Tama√±o de lote (por defecto 50)
- `--delay`: Retraso entre llamadas en segundos (por defecto 0.1)

### 3. Configurar Frontend

```bash
cd frontend
npm install
```

**Archivos actualizados:**
- `src/app/models/busqueda-semantica.ts` - Interfaces con nuevas m√©tricas
- `src/app/services/api.service.ts` - Cliente API
- `src/app/components/busqueda-semantica/` - Componente principal

---

## üíª Uso del Sistema

### Backend - API Endpoints

#### 1. B√∫squeda Sem√°ntica

**Endpoint:** `POST /api/busqueda/semantica/`

**Request Body:**
```json
{
  "texto": "env√≠os pesados entregados en Quito esta semana",
  "limite": 20,
  "modeloEmbedding": "text-embedding-3-small",
  "filtrosAdicionales": {
    "fechaDesde": "2025-01-01",
    "estado": "entregado",
    "ciudadDestino": "Quito"
  }
}
```

**Response:**
```json
{
  "consulta": "env√≠os pesados entregados en Quito esta semana",
  "resultados": [
    {
      "envio": { /* datos del env√≠o */ },
      "puntuacionSimilitud": 0.8524,
      "cosineSimilarity": 0.8524,
      "dotProduct": 125.67,
      "euclideanDistance": 12.34,
      "manhattanDistance": 45.67,
      "scoreCombinado": 0.9262,
      "fragmentosRelevantes": [
        "...Ciudad destino: Quito...",
        "...Peso: 15.5 kg..."
      ],
      "razonRelevancia": "Coincide con: ciudad Quito, estado Entregado",
      "textoIndexado": "HAWB: ABC123 | Comprador: Juan P√©rez | Ciudad destino: Quito..."
    }
  ],
  "totalEncontrados": 15,
  "tiempoRespuesta": 245,
  "modeloUtilizado": "text-embedding-3-small",
  "costoConsulta": 0.000012,
  "tokensUtilizados": 45,
  "busquedaId": 123
}
```

#### 2. Obtener Historial

**Endpoint:** `GET /api/busqueda/semantica/historial/`

```json
[
  {
    "id": 123,
    "consulta": "env√≠os pesados entregados en Quito",
    "fecha": "2025-11-20T10:30:00Z",
    "totalResultados": 15,
    "tiempoRespuesta": 245,
    "modeloUtilizado": "text-embedding-3-small",
    "costoConsulta": 0.000012,
    "tokensUtilizados": 45
  }
]
```

#### 3. M√©tricas

**Endpoint:** `GET /api/busqueda/semantica/metricas/`

```json
{
  "totalBusquedas": 150,
  "tiempoPromedioRespuesta": 280.5,
  "totalFeedback": 45,
  "feedbackPositivo": 38,
  "feedbackNegativo": 7,
  "totalEmbeddings": 1250
}
```

### Frontend - Uso del Componente

```html
<!-- B√∫squeda unificada (recomendado) -->
<app-busqueda-unificada></app-busqueda-unificada>

<!-- Solo b√∫squeda sem√°ntica -->
<app-busqueda-semantica></app-busqueda-semantica>
```

**Navegaci√≥n:**
```
http://localhost:4200/busqueda-unificada
```

---

## üìä M√©tricas de Similitud

### 1. Cosine Similarity (Similitud Coseno)

**F√≥rmula:** `cos(Œ∏) = (A ¬∑ B) / (||A|| √ó ||B||)`

**Rango:** [-1, 1]
- **1.0**: Vectores id√©nticos (m√°xima similitud)
- **0.0**: Vectores ortogonales (sin relaci√≥n)
- **-1.0**: Vectores opuestos

**Uso:** M√©trica principal, ignora la magnitud de los vectores.

**Ejemplo:**
```
Consulta: "env√≠os pesados"
Resultado 1: "HAWB: ABC | Peso: 25 kg" ‚Üí 0.85 (muy similar)
Resultado 2: "HAWB: XYZ | Peso: 2 kg"  ‚Üí 0.45 (poco similar)
```

### 2. Dot Product (Producto Punto)

**F√≥rmula:** `A ¬∑ B = Œ£(Ai √ó Bi)`

**Rango:** [0, ‚àû]
- **Mayor valor**: M√°s similar
- **0**: Sin similitud

**Uso:** Considera tanto direcci√≥n como magnitud.

### 3. Euclidean Distance (Distancia Euclidiana)

**F√≥rmula:** `d = sqrt(Œ£(Ai - Bi)¬≤)`

**Rango:** [0, ‚àû]
- **0**: Vectores id√©nticos
- **Mayor valor**: M√°s diferente

**Uso:** Distancia geom√©trica en espacio vectorial.

### 4. Manhattan Distance (Distancia Manhattan/L1)

**F√≥rmula:** `d = Œ£|Ai - Bi|`

**Rango:** [0, ‚àû]
- **0**: Vectores id√©nticos
- **Mayor valor**: M√°s diferente

**Uso:** Suma de diferencias absolutas.

### Comparaci√≥n de M√©tricas

| M√©trica | Mejor para | Ventajas | Desventajas |
|---------|------------|----------|-------------|
| **Cosine** | B√∫squeda sem√°ntica general | No afectada por magnitud, estable | Ignora longitud del vector |
| **Dot Product** | Cuando magnitud importa | R√°pido de calcular | Sensible a escala |
| **Euclidean** | Distancias geom√©tricas | Intuitiva | Sensible a outliers |
| **Manhattan** | Datos de alta dimensi√≥n | Menos sensible a outliers | Menos precisa |

### Umbrales Recomendados

```python
# Cosine Similarity
umbral_excelente = 0.8  # Muy relevante
umbral_bueno = 0.6      # Relevante
umbral_aceptable = 0.3  # M√≠nimamente relevante

# Sistema usa por defecto: 0.3
```

---

## üîß Generaci√≥n de Embeddings

### Proceso Autom√°tico

Los embeddings se generan autom√°ticamente cuando se crea un env√≠o:

```python
# En views.py y utils_importacion.py
def create(self, request, *args, **kwargs):
    envio = serializer.save()
    
    # Generar embedding autom√°ticamente
    try:
        generar_embedding_envio(envio)
    except Exception as e:
        # No falla la creaci√≥n si falla el embedding
        print(f"Advertencia: {e}")
```

### Contenido del Embedding

El texto indexado incluye:

```python
texto = " | ".join([
    f"HAWB: {envio.hawb}",
    f"Comprador: {envio.comprador.nombre}",
    f"Ciudad destino: {envio.comprador.ciudad}",
    f"Estado: {envio.get_estado_display()}",
    f"Fecha: {envio.fecha_emision}",
    f"Peso: {envio.peso_total} kg",
    f"Valor: ${envio.valor_total}",
    f"Productos: {descripciones}",
    f"Categor√≠as: {categorias}",
    f"Observaciones: {envio.observaciones}"
])
```

### Modelos Disponibles

| Modelo | Dimensiones | Costo/1M tokens | Velocidad | Precisi√≥n |
|--------|-------------|-----------------|-----------|-----------|
| **text-embedding-3-small** | 1536 | $0.02 | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê |
| **text-embedding-3-large** | 3072 | $0.13 | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **text-embedding-ada-002** | 1536 | $0.10 | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê |

**Recomendaci√≥n:** `text-embedding-3-small` para la mayor√≠a de casos.

---

## ‚ö° Optimizaci√≥n y Mejores Pr√°cticas

### 1. Costos

```python
# C√°lculo de costo estimado
tokens_promedio_por_envio = 100
costo_por_1k_tokens = 0.00002  # text-embedding-3-small

# Para 1000 env√≠os:
costo_total = (1000 * 100 / 1000) * 0.00002
# = 100 tokens * 0.00002 = $0.002 (0.2 centavos)

# Para 10,000 env√≠os: ~$2
```

**Estrategias de ahorro:**
- Generar embeddings solo una vez
- Usar `text-embedding-3-small` por defecto
- Implementar cach√© de resultados frecuentes
- Batch processing para generaci√≥n masiva

### 2. Velocidad

**Optimizaciones implementadas:**

```python
# 1. Limitar env√≠os a procesar
envios_queryset[:500]  # M√°ximo 500 env√≠os por b√∫squeda

# 2. Select related / Prefetch related
envios = Envio.objects.all()
    .select_related('comprador')
    .prefetch_related('productos')

# 3. √çndices en base de datos
class Meta:
    indexes = [
        models.Index(fields=['modelo_usado']),
        models.Index(fields=['fecha_generacion']),
    ]

# 4. Delay entre llamadas API
time.sleep(0.1)  # Evitar rate limits
```

### 3. Precisi√≥n

**Mejorar resultados:**

```python
# 1. Ajustar umbral de similitud
umbral_minimo = 0.3  # Por defecto
umbral_estricto = 0.6  # Para resultados m√°s precisos

# 2. Combinar m√∫ltiples m√©tricas
# Usar cosine como principal + euclidean para desempatar

# 3. Feedback del usuario
# Registrar qu√© resultados fueron relevantes
enviarFeedbackSemantico(resultado_id, es_relevante=True)

# 4. Regenerar embeddings peri√≥dicamente
python manage.py generar_embeddings_masivo --forzar
```

### 4. Escalabilidad

**Para grandes vol√∫menes (>10,000 env√≠os):**

```python
# 1. Usar √≠ndices vectoriales de pgvector
# CREATE INDEX ON envioembedding USING ivfflat (embedding_vector vector_cosine_ops);

# 2. Procesamiento as√≠ncrono
from celery import shared_task

@shared_task
def generar_embedding_async(envio_id):
    envio = Envio.objects.get(id=envio_id)
    generar_embedding_envio(envio)

# 3. Cach√© de Redis para b√∫squedas frecuentes
# cache.set(f"busqueda:{hash(consulta)}", resultados, timeout=3600)
```

### 5. Monitoreo

```python
# M√©tricas a monitorear:
- Tiempo promedio de b√∫squeda
- Tasa de √©xito (resultados encontrados / b√∫squedas)
- Costo acumulado por mes
- Feedback positivo vs negativo
- Embeddings generados vs total de env√≠os
```

---

## üìà M√©tricas del Sistema

### Precisi√≥n

```
Precisi√≥n = Resultados Relevantes / Total Resultados
```

**Objetivo:** > 80% de resultados relevantes

### Costo

```
Costo mensual estimado:
- 1000 env√≠os nuevos/mes: ~$0.20
- 10,000 b√∫squedas/mes: ~$2.00
Total: ~$2.20/mes
```

### Velocidad

```
Tiempos objetivo:
- Generaci√≥n embedding: < 500ms
- B√∫squeda (100 env√≠os): < 300ms
- B√∫squeda (1000 env√≠os): < 1000ms
```

---

## üêõ Troubleshooting

### Error: "OpenAI API key no configurada"

```bash
# Verificar .env
cat backend/.env | grep OPENAI

# Debe mostrar:
OPENAI_API_KEY=sk-proj-...
```

### Error: "pgvector extension not found"

```sql
-- Conectar a PostgreSQL
psql -U postgres -d equityDB

-- Verificar extensi√≥n
SELECT * FROM pg_extension WHERE extname = 'vector';

-- Si no existe, instalar
CREATE EXTENSION vector;
```

### Error: "No se generan embeddings autom√°ticamente"

```python
# Verificar que la funci√≥n est√° siendo llamada
# En views.py, agregar log:
print(f"Generando embedding para env√≠o {envio.hawb}")
generar_embedding_envio(envio)
print(f"Embedding generado exitosamente")
```

### B√∫squedas muy lentas

```python
# 1. Verificar n√∫mero de env√≠os procesados
print(f"Procesando {envios_queryset.count()} env√≠os")

# 2. Limitar a 500
envios_queryset[:500]

# 3. Verificar √≠ndices en BD
# python manage.py dbshell
# SELECT * FROM pg_indexes WHERE tablename = 'busqueda_envioembedding';
```

---

## ‚úÖ Checklist de Implementaci√≥n

- [ ] PostgreSQL con pgvector instalado y habilitado
- [ ] Variables de entorno configuradas (OPENAI_API_KEY)
- [ ] Dependencias instaladas (requirements.txt)
- [ ] Migraciones ejecutadas
- [ ] Embeddings generados para env√≠os existentes
- [ ] Prueba de b√∫squeda sem√°ntica funcionando
- [ ] Frontend actualizado con nuevas m√©tricas
- [ ] M√©tricas de costo/velocidad/precisi√≥n monitoreadas

---

## üìö Recursos Adicionales

- [OpenAI Embeddings Documentation](https://platform.openai.com/docs/guides/embeddings)
- [pgvector GitHub](https://github.com/pgvector/pgvector)
- [Django REST Framework](https://www.django-rest-framework.org/)
- [Angular Documentation](https://angular.io/docs)

---

## üéì Pr√≥ximos Pasos

1. **Implementar cach√© de resultados** para b√∫squedas frecuentes
2. **Agregar an√°lisis de sentimiento** en observaciones
3. **Dashboard de m√©tricas** en tiempo real
4. **Reentrenamiento peri√≥dico** de embeddings
5. **A/B testing** de diferentes modelos

---

**Desarrollado por:** Universal Box Development Team  
**Fecha:** Noviembre 2025  
**Versi√≥n:** 1.0.0

